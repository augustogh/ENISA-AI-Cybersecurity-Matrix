[
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Compromising AI inference's correctness – data",
    "Description": "This type of threats refers possible exploitations involving either data manipulation, or unintentional selection bias in raw data, or modification of labels and deletion or omission of labelled data items. It may also refer to compromising AI correctness by insertion of adversarial data in augmented data sets, as well as by means of interruption of training or modification of model parameters.",
    "Racional": "Manipulación de los datos que se usan en la inferencia, afectando la salida del modelo.\nDirectamente relacionada con la fase de inferencia.",
    "Confidentiality": "0",
    "Integrity": "|",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "0",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "0",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "1",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Bias introduced by data owners",
    "Description": "Data Owners may try to hide information that will be fed to the AI systems as part of their business interests. Moreover, they are also people that may be biased themselves, as they tend to be far from raw data and may be incapable of giving good data to the models. This type of threat can severely affect trustworthiness of AI systems.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "1",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "0",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Compromise and limit AI results",
    "Description": "This type of threat can emerge due to involuntary or unintentionally actions from Data Owners, that may hide data due to business secrets or by not recognizing its value; by AI/ML designers and engineers, that can intentionally tamper or, due to lack of experience, miss to include data. This threat may also be related to AI/ML service users not being able to understand the model capabilities and/or results.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "1",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "0",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Compromising ML inference's correctness algorithms",
    "Description": "Threats to the availability of the ML training algorithm, as well as threats that aim at compromising the training algorithm to adversely affect the desired accuracy.",
    "Racional": "Alteración de los algoritmos que procesan las entradas en tiempo de inferencia.\nAfecta la lógica de inferencia.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "0",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "1",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Disclosure of personal information",
    "Description": "At all stages of the AI lifecycle, disclosure of personal information (either directly or by means of correlation) is a noteworthy threat. The threat is particularly manifested in the absence of verified data accuracy of sources, lack of data randomization, lack of pseudonymity mechanisms , etc.",
    "Racional": "Se refiere a la exposición directa de datos personales (PII) durante el procesamiento, almacenamiento o salida del sistema de IA.\nPuede ocurrir por fallos de configuración, falta de anonimización, errores en salidas (Sensitive model output), o accesos indebidos.\n (6.4 Membership inference) \n- Ataque específico que busca determinar si un registro concreto estuvo en el dataset de entrenamiento.\n- Es una inferencia estadística, no una fuga directa.\n\nDiferencia clave:\n- Disclosure of personal information = fuga directa (por salida, logs, mala configuración).\n- Membership inference = fuga indirecta (por inferencia a partir de respuestas del modelo).\nNo corresponde a 6.4, porque no implica inferencia estadística.\n------\nMembership inference revela si un registro estuvo en el entrenamiento.\nEsta amenaza se manifiesta particularmente en la ausencia de verificación de la precisión de las fuentes de datos, la falta de aleatorización de datos y la ausencia de mecanismos de seudonimización, entre otros.\nLa divulgación de información personal puede ocurrir en todas las fases:\n- Recolección y preprocesamiento: datos sin anonimización.\n- Entrenamiento: datasets mal gestionados.\n- Inferencia: salidas que revelan PII (Sensitive model output).\n\nDisclosure of personal information // es una amenaza general que incluye fugas directas\n└── Membership inference (ISO 6.4) // es una técnica que puede causar la amenaza si el dato inferido es personal. no es la técnica más común para esta amenaza\n---------------------------------------------------------------------\nDisclosure of personal information (en casos específicos)\n- Cuando la fuga ocurre por inferencia indirecta (no por salida directa).\n- Ejemplo: Inferir condiciones médicas a partir de predicciones.\n- Correspondencia indirecta: Puede ser consecuencia de model inversion.\nModel inversion (6.6) es una técnica que puede derivar en Disclosure of personal information, pero esta última también incluye otros vectores (p.ej., errores de configuración, falta de seudonimización).",
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "1",
    "Environment /Tools": "1",
    "Processes": "1",
    "Actors": "1",
    "Models": "1",
    "Data": "1",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "0",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Erroneous configuration of models",
    "Description": "This type of threat materializes when models are used recklessly by end users (but also AI experts) without proper consideration of contextual factors that may not fit with the phenomenon being analyzed. If there is a mismatch between the goal and the model this may result in biases and discriminations or bad performance in general. Lack of expertise and proper knowledge of AI models’ operation are the main cause of these erroneous configurations.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "1",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "0",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "1",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Compromising ML training augmented data",
    "Description": "Threats to augmented datasets due to inconsistency with the training set they are derived from, and specifically when highly diverse, automatically generated data are added to a data set of collected data, which are very consistent but highly representative of their application domain, so there would be no need to limit overfitting. Enriching data always entails some risks. This threat can lead to non- satisfaction of functional requirements, i.e. poor inference.",
    "Racional": "La amenaza descrita implica:\n\nInconsistencias entre datos originales y datos aumentados, especialmente si los datos generados automáticamente son muy diversos y no representan bien el dominio de aplicación.\nEsto puede provocar:\nInferencias pobres o no funcionales.\nSobreajuste o subajuste no detectado.\nDesviaciones del comportamiento esperado del modelo.\nAunque no siempre es malicioso, si el enriquecimiento se realiza sin validación, puede:\n\nFacilitar la inserción de datos manipulados.\nOcultar patrones adversariales en los datos generados.\nProvocar comportamientos no deseados en inferencia.\nEsto se alinea con la definición de data poisoning en la ISO/IEC DIS 27090, sección 6.2, si el enriquecimiento es malicioso o no controlado.\n\nConclusión:\nEl enriquecimiento inconsistente de datos puede ser una vía para data poisoning, especialmente si los datos generados son maliciosos o no se validan adecuadamente. Por tanto, sí está relacionada, aunque no siempre implica una intención maliciosa.\n\nEsta amenaza ocurre en:\n\nFase de entrenamiento: Cuando se combinan datos reales con datos aumentados sin control de calidad.\nTransferencia de modelos: Si se importa un modelo entrenado con datos enriquecidos de baja calidad.\nReentrenamiento continuo: Si el sistema incorpora datos generados automáticamente sin validación.\nNo suele ocurrir en la fase de inferencia, aunque sus efectos se manifiestan allí (por ejemplo, inferencias erróneas o no confiables).",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "1",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Compromising feature selection",
    "Description": "This threat refers to performance degradation of feature selection algorithms by delivering feature sets that are strongly predictive only for some for some classes, neglecting other features that are needed to discriminate difficult classes.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Compromise of data brokers/providers",
    "Description": "This threat refers to compromising data brokers/providers to influence the machine learning process as they can deliberately or accidentally manipulate the data sent to the AI process, in several different ways: poisoning-via- insertion of malicious data; deleting registries to eliminate features either by changing the data, removing part of it or adding new registries. In addition, sometimes the mere data availability prevails over any consideration on data quality, with the risk that learning models are fed with data streams that do not reflect the statistical characteristics of a phenomenon and determining likely biases in the subsequent decisional processes.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "1",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Compromise of model frameworks",
    "Description": "Model frameworks fail when are misconfigured or offer additional attack vectors with respect to\ntraditional software, firmware and hardware environments. The ML platform’s data volume and processing requirements mean that the workloads are often handled on the cloud, adding another level of complexity and vulnerability.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Compromise privacy during data operations",
    "Description": "Data modification or erroneous handling during Processes like Data Exploration' or Pre- Processing may lead to unintentional data breaches respectively and accordingly lead to legal concerns over privacy breaches.",
    "Racional": "violación de la privacidad durante el procesamiento de datos en sistemas de IA, incluyendo:\n- Uso indebido de datos personales.\n- Falta de técnicas de preservación de privacidad (como anonimización o cifrado).\n- Exposición de datos sensibles durante inferencia, entrenamiento o despliegue\n\nCorrespondencia parcial\nJustificación: Ambas amenazas están relacionadas con la exposición de información sensible, pero:\nISO 6.10 se enfoca en la entrada del modelo como canal de fuga.\nENISA aborda un espectro más amplio de violaciones de privacidad durante operaciones con datos, que puede incluir entradas, salidas, almacenamiento, y procesamiento.\nPor tanto, la amenaza ISO es un subconjunto específico de la categoría más general de ENISA.\n\nCompromise privacy during data operation (ENISA)\n└── Model input leak (ISO 27090 6.10) [Parcial]",
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Label manipulation or weak labelling",
    "Description": "This threat refers to supervised learning systems, which not infer correctly due to wrong or imprecise data labels. Messing with the labels may introduce the same effects of threats that are pertinent to adversaries attacking the labelling process.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Lack of sufficient representation in data",
    "Description": "Raw data assets fail when they are not sufficiently representative of the domain or unfit for the AI business goal, e.g. due to sample size and population characteristics. Data size does not always imply representativeness. If data selection is biased towards some elements that have similar characteristics (selection bias) then even a large sample will not deliver representative data. Assessment of data representativeness cannot be done a priori; it is only possible after identifying the targeted population and the purpose for collecting the data. Selection bias can be alleviated by employing balanced sampling techniques. Correction of existing data sets does, however, require information regarding the existence and nature of the bias; when selection bias is unknown to the AI model designer, no correction-based approaches to the inference process are possible.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Manipulation of labelled data",
    "Description": "Unintentional threats to labelled data items occur when enough numbers of labels and data are deleted/omitted by mistake, when a sufficient number of spurious labelled data is included into the data set, or when enough labels are modified Since the labelled data set is used for the purpose of training a ML model, all such modifications affect the model training and inference (e.g. shifting the models’ classification boundary).",
    "Racional": "La amenaza descrita como manipulación de datos etiquetados incluye:\n\nEliminación u omisión accidental de etiquetas.\nInclusión de datos etiquetados espurios.\nModificación de etiquetas existentes.\nEstas acciones afectan directamente el proceso de entrenamiento del modelo, provocando:\n\nDesplazamiento de los límites de decisión del modelo.\nErrores sistemáticos en inferencia.\nReducción de la precisión y confiabilidad.\n\nEsta amenaza ocurre típicamente en:\n\nFase de entrenamiento inicial: Cuando se construye el modelo por primera vez.\nFase de reentrenamiento o aprendizaje continuo: Si el sistema incorpora nuevos datos etiquetados sin validación robusta.\nTransferencia de aprendizaje: Si se importa un modelo preentrenado con etiquetas manipuladas.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Misconfiguration or mishandling of AI system",
    "Description": "AI designers and developers may unintentionally expose data and models or may even misconfigure an AI system by mistake. Data confidentiality and trustworthiness are the main impacted security properties.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "1",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Mishandling of statistical data",
    "Description": "This may happen, for instance, if maximum likelihood predictions are drawn from the sample, correctly reflecting the way the majority of individuals express a specific parameter that may not mirror the way the minority will be affected by the prediction. Also, other forms of unintended bias may take place. For instance, in ranking algorithms even if parameters under analysis may be ranked fairly and in the correct order, the rewards allocated to each ‘slot’ (click through rates, impressions or any other sort of share of ‘good’ to allocate) may not be fairly distributed, with limited possibility to rebalance such uneven situations.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "0",
    "9. Transfer Learning": "0",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "ML Model Performance Degradation",
    "Description": "This performance of an AI’s system ML model may degrade due to the data governance policy,\nby omission or by corruption due to system crashes or loss of network connectivity.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Online system manipulation",
    "Description": "This is related to model replacement by a backdoored model by mistake, used for targeted or non-targeted attack. This can be the result of unintended actions by Actors like Cloud Providers during Processes like model training or transfer.",
    "Racional": "Manipulación activa del sistema en tiempo real, incluyendo entradas maliciosas.\nPuede incluir manipulación de inputs en producción.",
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "1",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Unintentional Damage",
    "Threat": "Reducing data accuracy",
    "Description": "This threat refers to the reduction of the degree of data accuracy, by directly modifying the data or by mixing datasets with highly different degrees of quality.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Physical attacks",
    "Threat": "Communication networks tampering",
    "Description": "Tampering of communication networks may lead to their unavailability and thus is a major threat that may be exploited by adversaries. The corresponding outages may lead to delays in decision-making, delays in the processing of data streams and entire AI systems being placed offline. Moreover, side-channel attacks may expose private and sensitive information that traverses communication networks.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Physical attacks",
    "Threat": "Errors or timely restrictions due to non-reliable data infrastructures",
    "Description": "This type of threat is related to data and computational exposure and/or inadequate capacity that may expose data and compromise privacy preservation.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Physical attacks",
    "Threat": "Infrastructure/system physical attacks",
    "Description": "Physical attacks against infrastructure (IT and corporate services) that supports AI systems operation and maintenance. Manifestation of this threat leads to degraded performance and even unavailability.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Physical attacks",
    "Threat": "Model Sabotage",
    "Description": "Sabotaging the model is a nefarious threat that  refers to exploitation or physical damage to hardware hosting libraries and machine learning platforms that host or supply AI/ML services and systems",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Physical attacks",
    "Threat": "Sabotage",
    "Description": "Sabotage involves intentionally destroying or maliciously affecting the IT infrastructure that supports AI systems.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Outages",
    "Threat": "Communication networks outages",
    "Description": "Outages to communication networks may adversely influence the performance and operation of AI systems. Such outages may lead to delays in decision-making, delays in the processing of data streams and entire AI systems being placed offline.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Outages",
    "Threat": "Infrastructure/system outages",
    "Description": "Outages to infrastructure (IT and corporate services) that supports AI systems’ operation and maintenance. Manifestation of this threat leads to degraded performance and even unavailability.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "ML Model integrity manipulation",
    "Description": "This threat refers to manipulation of the ML model by delivering output values that were not generated based on its internal parameters or by delivering overtly biased or useless outputs (e.g. constant, or undistinguishable from random noise).",
    "Racional": "Aunque ENISA no utiliza este término como categoría explícita, el concepto aparece en el contexto de amenazas que comprometen la integridad del modelo de IA, incluyendo manipulaciones durante el entrenamiento, tuning, transferencia o despliegue. Estas amenazas buscan alterar el comportamiento esperado del modelo, ya sea por sabotaje, sesgo inducido o corrupción de parámetros\n✅ Directa\n\nJustificación: Ambas amenazas describen el mismo vector: alteración intencionada del modelo de IA para comprometer su integridad. El término “ML Model integrity manipulation” en ENISA abarca directamente lo que ISO denomina “Direct model poisoning”, ya que ambas se refieren a ataques que modifican el modelo para inducir fallos o comportamientos maliciosos.\nML Model integrity manipulation (ENISA)\n└── Direct model poisoning (ISO 27090 6.7) [Directa]",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "0",
    "6. Model Selection / Building": "0",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Manipulation of model tuning",
    "Description": "Adversaries may fine-tune hyper-parameters ",
    "Racional": "Alteración deliberada de parámetros de ajuste (hiperparámetros, configuración de entrenamiento) para degradar el rendimiento o introducir comportamientos maliciosos.\nImpacto: Integridad del modelo.\nFase: Entrenamiento / fine-tuning.\n\nEnvenenamiento directo del modelo (6.7 Direct model poisoning) también describe el mismo vector: manipulación directa del modelo (no de datos) durante el entrenamiento.\nRelación con otras amenazas:\n- Manipulation of optimization algorithm también cae en 6.7.\n- Model poisoning (ENISA) es la categoría general que agrupa estas técnicas.\n\n“Manipulation of model tuning” (ENISA) es una técnica específica dentro de la categoría “Direct model poisoning” (ISO 6.7).\n",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "0",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "ML model confidentiality",
    "Description": "This threat refers to exploitation of the ML model to leak (in its outputs or otherwise) some information about its internal parameters or performance to un-authorized parties.",
    "Racional": "ML model confidentiality comprende cualquier fuga de información del modelo, incluyendo parámetros, estructura, rendimiento y datos.(general). Esto abarca tanto ataques directos (exfiltración) como indirectos (inferencia).\nInferencia de pertenencia (6.4 Membership inference) es una técnica concreta que busca revelar si un dato específico fue parte del entrenamiento, lo cual compromete la confidencialidad del modelo y la privacidad de los datos.(específica). \nDetermina si un registro estuvo en el dataset de entrenamiento.\n\nMembership inference (ISO 6.5) es una subcategoría de Data inference, y ambas son parte de la amenaza general ML model confidentiality.\nML model confidentiality\n├── Data inference \n│   └── Membership inference  (ISO 6.5)\n\nDescripción: Agrupa ataques como Model Disclosure, Model Exfiltration, Unauthorized access to models’ code, Data inference.\nObjetivo: Robar el modelo o inferir información sensible.\nCorrespondencia: Sí aplica directamente a 6.5, porque incluye Model Exfiltration como subamenaza y comparte el mismo objetivo (robo del modelo).  también a ISO (6.4, 6.6, 6.8).\n------------------------------------------------------------\n(6.4 Membership inference) define un ataque específico: Determinar si un dato concreto estuvo en el conjunto de entrenamiento.\n\nRelación: \nMembership inference es una técnica dentro de ML Model Confidentiality, pero esta última también cubre ataques más amplios como Model Exfiltration (ISO 6.5) y Direct model theft (ISO 6.8).\n\nML model confidentiality\n├── Data inference \n│   └── Membership inference  (ISO 6.5)\n├── Inferencia de pertenencia (6.4 Membership inference)\n├── Model Exfiltration (ISO 6.5)\n└── Direct model theft (ISO 6.8)\n\nML Model Confidentiality ≠ (6.4 Membership inference), sino que incluye 6.4, 6.5 y 6.8 (y parcialmente 6.6 Model inversion).\nML Model Confidentiality incluye Membership inference (6.4), pero no se limita a ella.",
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": null,
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "0",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "1",
    "6.5": "1",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Transferability of adversarial attacks",
    "Description": "ML and deep learning models are mostly based on an inductive approach to problem solving, as opposed to the deductive approach of traditional mathematical modelling. This means that experience matters and not always it can be given for granted that ML models can be smoothly transferred and applied in a new scenario and for a new AI application. This threat refers to adversarial examples that may be transferred to AI/ML applications and Environment/Tools like AI/ML libraries and machine learning platforms.",
    "Racional": "Uso de ejemplos adversarios diseñados para un modelo y aplicados a otro.\nSe ejecuta en producción, aprovechando similitudes entre modelos.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "0",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "1",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Access Control List (ACL) manipulation",
    "Description": "In AI data collection scenarios, group-based ACLs for datasets may fail when the nesting of large groups is changed. Given a data set and a group of sources Sensor_Group_A which has been granted access to update it, it is easy to check if an individual user or sensor is a member of Sensor_Group_A and inherits the corresponding permissions. However, if Sensor_Group_A is joined as a member to many other groups, inherited permissions become difficult to check for each sensor and escalation of privileges of untrusted sources may result. The threat involves Implicit privilege elevation attacks take advantage of group nesting modifications to upscale access permissions for specific users.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Adversarial examples",
    "Description": "Targeting the inference phase of ML and deep learning systems that AI is based on is one of the most prominent and highly publicized threats. Adversarial examples refer to data that include perturbations that are imperceptible to the human eye, but that can have an impact on the effectiveness and performance of ML models. Adversarial examples son entradas manipuladas que se introducen durante la inferencia, es decir, cuando el modelo ya está desplegado y en uso. El atacante no necesita acceso al entrenamiento, sino que interactúa con el modelo como un usuario más, pero con entradas diseñadas para engañar la clasificación o decisión del sistema. Evaluation and Testing es donde deberían detectarse estas vulnerabilidades antes del despliegue (testing adversarial).",
    "Racional": "Entradas manipuladas con perturbaciones imperceptibles para engañar al modelo. \nLos ejemplos adversariales se refieren a datos que incluyen perturbaciones que son imperceptibles para el usuario, pero que pueden tener un impacto en la efectividad y el rendimiento de los modelos. es decir, cuando el modelo ya está desplegado y en uso. El atacante no necesita acceso al entrenamiento, sino que interactúa con el modelo como un usuario más, pero con entradas diseñadas para engañar la clasificación o decisión del sistema. Evaluación y Pruebas es donde deberían detectarse estas vulnerabilidades antes del despliegue (testing adversarial).\n\nSe manifiesta en la fase de inferencia, que forma parte de la etapa de operación y monitorización del ciclo de vida de la IA. Esta es la fase en la que el modelo ya ha sido entrenado, validado y desplegado, y está siendo utilizado en producción.\n\nEntradas manipuladas con perturbaciones imperceptibles para engañar al modelo.\n\nSin embargo, hay una relación crítica con la fase de evaluación y testing, ya que:\nDurante la evaluación (antes del despliegue), deberían aplicarse técnicas de testing adversarial para identificar si el modelo es vulnerable a este tipo de entradas manipuladas.\nSi no se detectan en esa fase, los adversarial examples pueden explotarse en producción, lo que representa un riesgo operativo.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "1",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "1",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Backdoor/insert attacks on training datasets",
    "Description": "Introduce spurious inferences. Attackers introduce special trigger patterns in part of the training data, and presenting the trigger in the inference phase will cause targeted misclassifications. For example, an attacker can introduce in the training data of an image classifier connected to a surveillance camera an example including a certain pixel pattern and the label “policeman”. One the classifier is trained and deployed, the attacker wears a t-shirt with that pattern and passes by the camera with a gun in hand without triggering any alarm.",
    "Racional": "Los ataques de puerta trasera (backdoor attacks) en conjuntos de entrenamiento son una forma especializada de data poisoning, donde:\n\nEl atacante inserta patrones específicos en los datos de entrenamiento junto con etiquetas manipuladas.\nEl modelo aprende a asociar ese patrón con una clase específica.\nEn producción, al presentar el patrón (por ejemplo, una camiseta con un diseño), el modelo realiza una clasificación errónea predecible.\nEsto se alinea con la definición de data poisoning en la ISO/IEC DIS 27090, sección 6.2:\n\n“Manipulación maliciosa de los datos de entrenamiento con el fin de influir negativamente en el aprendizaje del modelo…”\n\nEste tipo de ataque ocurre en:\n- Fase de entrenamiento inicial: Cuando se introduce el patrón malicioso en los datos.\n- Transferencia de modelos: Si se importa un modelo preentrenado con backdoors.\n- Reentrenamiento continuo: Si el sistema incorpora nuevos datos sin validación robusta.\n- La activación del ataque ocurre en la fase de inferencia, cuando el patrón es presentado al modelo.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "1",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Compromising AI inference's correctness – data",
    "Description": "This type of threats refers to possible exploitations involving either data manipulation, or selection bias in raw data, or modification of labels and deletion or omission of labelled data items. It may also refer to compromising AI correctness by insertion of adversarial data (poisoned/manipulated) in augmented data sets, as well as by means of interruption of training or modification of model parameters.",
    "Racional": "Este tipo de amenazas se refiere a posibles explotaciones que implican manipulación de datos, sesgo de selección en datos brutos, modificación de etiquetas y eliminación u omisión de datos etiquetados. También puede comprometer la precisión de la IA mediante la inserción de datos adversarios (envenenados/manipulados) en conjuntos de datos aumentados, así como mediante la interrupción del entrenamiento o la modificación de los parámetros del modelo.\nManipulación de los datos que se usan en la inferencia, afectando la salida del modelo.\nDirectamente relacionada con la fase de inferencia.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "1",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Compromise and limit AI results",
    "Description": "This type of threat can emerge due to involuntary or unintentionally actions from Data Owners, that may hide data due to business secrets or by not recognizing its value; by AI/MLdesigners and engineers, that can intentionally tamper or, due to lack of experience, miss to include data. This threat may also be related to AI/ML service users not being able to understand the model capabilities and/or results.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "1",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Compromising ML inference's correctness algorithms",
    "Description": "Threats to the availability of the ML training algorithm, as well as threats that aim at compromising the training algorithm to adversely affect the desired accuracy.",
    "Racional": "Alteración de los algoritmos que procesan las entradas en tiempo de inferencia.\nAfecta la lógica de inferencia.",
    "Confidentiality": "º",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "1",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "1",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Compromising ML pre-processing",
    "Description": "Flaws or defects of the data and metadata schemata greatly influence the quality of the analysis by applications that use the data. In AI applications, a flawed schema will negatively impact on the quality of the ingested information. Flaws often result from of inconsistencies in the use of modeling methodologies, but may also depend on intentional schema poisoning, i.e. any manipulation of a schema intended to compromise the programs that ingest or pre- process data that use it. It is also possible for adversaries to mount Schema-based denial of service attacks, which modify the data schema so that it does not contain required information for subsequent processing.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Compromising ML training augmented data",
    "Description": "Threats to augmented datasets due to inconsistency with the training set they are derived from, and specifically when highly diverse, automatically generated data are added to a data set of collected data, which are very consistent but highly representative of their application domain, so there would be no need to limit overfitting. Enriching data always entails some risks. This threat can lead to non- satisfaction of functional requirements, i.e. poor inference.",
    "Racional": "puede estar relacionada dependiendo del origen y control del proceso de enriquecimiento.\n\nLa amenaza descrita implica:\n\nInconsistencias entre datos originales y datos aumentados (por ejemplo, generados automáticamente).\nRiesgo de introducir ruido, sesgos o patrones irrelevantes que afecten el aprendizaje.\nPosibilidad de que el modelo no cumpla requisitos funcionales, como precisión o robustez.\nEsto no siempre es malicioso, pero si el enriquecimiento se realiza sin validación, puede:\n\nFacilitar la inserción de datos manipulados.\nOcultar patrones adversariales en los datos generados.\nProvocar comportamientos no deseados en inferencia.\nConclusión:\nEl enriquecimiento inconsistente de datos puede ser una vía para data poisoning, especialmente si los datos generados son maliciosos o no se validan adecuadamente. Por tanto, sí está relacionada, aunque no siempre implica una intención maliciosa.\n\nEsta amenaza ocurre en:\n\nFase de entrenamiento: Cuando se combinan datos reales con datos aumentados sin control de calidad.\nTransferencia de modelos: Si se importa un modelo entrenado con datos enriquecidos de baja calidad.\nReentrenamiento continuo: Si el sistema incorpora datos generados automáticamente sin validación.",
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Compromising ML training validation data",
    "Description": "This threat refers to shortening the training of the ML model dramatically by compromising the integrity of the validation dataset. It also includes, generation of adversarial validation data that are quite different from genuine training set data",
    "Racional": "La amenaza descrita implica:\n\nCompromiso de la integridad del conjunto de validación, ya sea por:\nReducción artificial del tamaño del conjunto.\nInclusión de datos adversariales que no representan el conjunto de entrenamiento.\nEsto puede provocar:\nEvaluaciones incorrectas del rendimiento del modelo.\nSobreajuste o subajuste no detectado.\nDespliegue de modelos inseguros o ineficaces.\nEste tipo de manipulación encaja con la definición de data poisoning en la ISO/IEC DIS 27090, sección 6.2, ya que:\n\n“Manipulación maliciosa de los datos de entrenamiento con el fin de influir negativamente en el aprendizaje del modelo…”\n\nAunque el conjunto de validación no se usa directamente para entrenar, su manipulación afecta indirectamente el proceso de aprendizaje, al distorsionar la evaluación del modelo.\n\nConclusión:\nLa manipulación del conjunto de validación es una forma indirecta de data poisoning, que compromete la seguridad funcional y la confiabilidad del modelo.\n\nEsta amenaza ocurre en:\n\nFase de entrenamiento: Cuando se selecciona o construye el conjunto de validación.\nTesting y validación: Si se usan datos adversariales para evaluar el modelo.\nTransferencia de modelos: Si se importan modelos validados con conjuntos manipulados.\nReentrenamiento continuo: Si el sistema actualiza su validación sin controles robustos.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Compromise of data brokers/providers",
    "Description": "This threat refers to compromising data brokers/providers to influence the machine learning process as they can deliberately or accidentally manipulate the data sent to the AI process, in several different ways: poisoning-via- insertion of malicious data; deleting registries to eliminate features either by changing the data, removing part of it or adding new registries. In addition, sometimes the mere data availability prevails over any consideration on data quality, with the risk that learning models are fed with data streams that do not reflect the statistical characteristics of a phenomenon and determining likely biases in the subsequent decisional processes.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "1",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Compromise of model frameworks",
    "Description": "Model frameworks fail when are misconfigured or offer additional attack vectors with respect to traditional software, firmware and hardware enviroments. The ML platform’s data volume and processing requirements mean that the workloads are often handled on the cloud, adding another level of complexity and vulnerability. Moreover, the threat of backdoors in libraries is also evident, similarly to the potential threats of attacks on model input and output data can be performed on the hardware, firmware, Operating System (OS) and software level.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Corruption of data indexes",
    "Description": "Data indexes threats manifest when their content becomes corrupted. Corruption may be the result of an attack, or due to system crashes or loss of network connectivity during index replication. The same events may cause interruptions of index construction tasks, bringing a partially built (and therefore defective) index to production. Also, running out of storage capacity during indexing or replication may cause an entire data index to be deleted. Denial-of-service attacks to indexes intentionally corrupt data indexes to decrease the performance of data access. Additionally, timing attacks to indexes use access time to public items before and after inserting them (which depend on the index content) to infer the presence and size of inaccessible data items.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Data poisoning",
    "Description": "This threat relates to the injection of erroneous/tampered/wrong data in the training set or the validation set by either getting legitimate access or illegitimate one through exploiting poor authentication/authorization mechanisms. The aim is to adversely affect operation of the AI system.",
    "Racional": "Esta amenaza se relaciona con la inyección de datos erróneos, alterados o incorrectos en el conjunto de entrenamiento o el conjunto de validación, ya sea obteniendo acceso legítimo o ilegítimo mediante el aprovechamiento de mecanismos de autenticación o autorización deficientes. El objetivo es afectar negativamente el funcionamiento del sistema de IA.\n\nLa descripción que proporcionas:\n\n“Injection of erroneous/tampered/wrong data in the training set or the validation set… to adversely affect operation of the AI system”\n\ncorresponde directamente con la definición formal de data poisoning en la ISO/IEC DIS 27090, sección 6.2, que establece:\n\n“Manipulación maliciosa de los datos de entrenamiento con el fin de influir negativamente en el aprendizaje del modelo…”\n\nAmbos casos incluyen:\n\nInserción de datos corruptos o manipulados.\nAcceso legítimo o ilegítimo mediante explotación de debilidades en autenticación/autorización.\nImpacto directo en el comportamiento del modelo, afectando su seguridad funcional.\nConclusión:\nEsta amenaza es una forma directa y completa de data poisoning, tal como se define en ISO 27090, ENISA y OWASP AI Top 10.\n\nAunque el ataque se origina principalmente en:\n\nFase de entrenamiento inicial, donde se inyectan los datos maliciosos,\ntambién puede extenderse a:\n\nFase de validación, si se manipulan los datos usados para evaluar el modelo.\nReentrenamiento continuo, si el sistema incorpora nuevos datos sin controles robustos.\nTransferencia de modelos, si se importan modelos entrenados con datos envenenados.\nConclusión:\nEs una amenaza que se origina en el entrenamiento, pero puede afectar múltiples fases del ciclo de vida del sistema de IA, especialmente si no hay trazabilidad ni validación de datos.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "1",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "1",
    "6.11": "1",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Data Tampering",
    "Description": "Actors like AI/ML designers and engineers can deliberately or unintentionally manipulate and expose data. Data can also be manipulated during the storage procedure and by means of some processes like feature selection. Besides interfering with model inference, this type of threat can also bring severe discriminatory issues by introducing bias.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "1",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "1",
    "6.7": "0",
    "6.8": "1",
    "6.9": "1",
    "6.10": "0",
    "6.11": "1",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "DDoS",
    "Description": "Distributed Denial of Service attacks may be utilized by adversaries to reduce the availability of online IT systems and distributed file systems (e.g. cloud storage) used to support AI systems and their operation.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Elevation of Privilege",
    "Description": "These threats refers to exploiting trained and tuned models to gain access to parameters values and even to understand whether some data was part of the data set used.",
    "Racional": "Elevation of Privilege \n- es un ataque orientado a escalar privilegios en el sistema\n- ocurre principalmente en la fase de operación e inferencia del ciclo de vida de los sistemas de IA, cuando el modelo está desplegado.\nDescripción: Ataque para obtener privilegios más altos en el sistema (p.ej., pasar de usuario a administrador).\nObjetivo: Control del sistema, no necesariamente extracción del modelo.\nRelación con 6.5: Indirecta. Elevation of Privilege puede ser un vector habilitador para realizar model exfiltration (por ejemplo, si el atacante obtiene permisos para acceder al repositorio del modelo), pero no es en sí misma una amenaza de exfiltración.\nConclusión: No se clasifica directamente en 6.5, pero se considera un ataque previo o facilitador.\n\nExfiltración del modelo (6.5 Model exfiltration)\nModel Exfiltration puede ser una técnica utilizada dentro de un ataque de Elevation of Privilege, si el objetivo es obtener acceso a información que normalmente estaría restringida.\nElevation of Privilege es más amplio: incluye cualquier forma de escalada de acceso, no solo la reconstrucción del modelo.\nModel Exfiltration es más específico: se centra en la reconstrucción del modelo a través de su comportamiento observable.\n\n",
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Insider threat",
    "Description": "AI designers and developers may deliberately expose data and models for a variety of reasons, e.g. revenge or extortion. Integrity, data confidentiality and trustworthiness are the main impacted security properties.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "1",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Introduction of selection bias",
    "Description": "Data owners may introduce selection bias on purpose when publishing raw data in order to adversely affect inference to be drawn on the data.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Label manipulation or weak labelling",
    "Description": "This threat refers to supervised learning systems, which not infer correctly due to wrong or imprecise data labels. If adversaries can only modify the training labels with some or all knowledge of the target model, they need to find the most vulnerable labels. Random perturbation of labels is one possible attack, while additionally there is the case of adversarial label noise (intentional switching of classification labels leading to deterministic noise, an error that the model cannot capture due to its generalization bias).",
    "Racional": "La amenaza descrita implica:\n\nModificación intencional o aleatoria de etiquetas en sistemas de aprendizaje supervisado.\nIntroducción de ruido adversarial en las etiquetas, lo que genera errores sistemáticos que el modelo no puede corregir debido a su sesgo de generalización.\nEl atacante puede tener conocimiento parcial o total del modelo, lo que le permite identificar las etiquetas más vulnerables.\nEsto se alinea completamente con la definición de data poisoning en la ISO/IEC DIS 27090, sección 6.2, que contempla:\n\n“Manipulación maliciosa de los datos de entrenamiento con el fin de influir negativamente en el aprendizaje del modelo…”\n\nConclusión:\nLa manipulación de etiquetas es una forma específica de data poisoning, orientada a comprometer la integridad del aprendizaje supervisado y provocar clasificaciones erróneas sistemáticas.\n\nLa amenaza descrita implica:\n\nModificación intencional o aleatoria de etiquetas en sistemas de aprendizaje supervisado.\nIntroducción de ruido adversarial en las etiquetas, lo que genera errores sistemáticos que el modelo no puede corregir debido a su sesgo de generalización.\nEl atacante puede tener conocimiento parcial o total del modelo, lo que le permite identificar las etiquetas más vulnerables.\nEsto se alinea completamente con la definición de data poisoning en la ISO/IEC DIS 27090, sección 6.2, que contempla:\n\n“Manipulación maliciosa de los datos de entrenamiento con el fin de influir negativamente en el aprendizaje del modelo…”\n\nConclusión:\nLa manipulación de etiquetas es una forma específica de data poisoning, orientada a comprometer la integridad del aprendizaje supervisado y provocar clasificaciones erróneas sistemáticas.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Manipulation of data sets and data transfer process",
    "Description": "These threats are seen in context of storage of data sets in infrastructures provided by third parties, which make them remotely accessible. The threat refers to manipulation and tampering of the data stored and manipulation of the data transfer process.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Manipulation of labelled data",
    "Description": "Threats to labelled data items occur when enough labels and data are deleted/omitted, when a sufficient number of spurious labelled data is included into the data set, or when enough labels are modified Since the labelled data set is used for the purpose of training a ML model in the supervised setting, all such modifications affect the model training and inference (e.g., shifting the m boundary)",
    "Racional": "La amenaza descrita implica:\n\nEliminación u omisión de etiquetas o datos.\nInclusión de datos etiquetados espurios.\nModificación de etiquetas en cantidad suficiente como para alterar el comportamiento del modelo.\nEsto afecta directamente el proceso de entrenamiento en sistemas de aprendizaje supervisado, provocando:\n\nDesplazamiento de los límites de decisión del modelo.\nErrores sistemáticos en inferencia.\nReducción de la precisión y confiabilidad.\nEsta descripción se alinea completamente con la definición de data poisoning en la ISO/IEC DIS 27090, sección 6.2, que contempla:\n\n“Manipulación maliciosa de los datos de entrenamiento con el fin de influir negativamente en el aprendizaje del modelo…”\n\nConclusión:\nLa manipulación de datos etiquetados es una forma directa de data poisoning, que compromete la integridad del aprendizaje supervisado y la seguridad funcional del sistema de IA.\n\nEsta amenaza ocurre en:\n\nFase de entrenamiento inicial: Cuando se construye el conjunto de datos etiquetados.\nReentrenamiento continuo: Si el sistema incorpora nuevos datos sin validación robusta.\nTransferencia de modelos: Si se importa un modelo preentrenado con etiquetas manipuladas.\nNo suele ocurrir en la fase de inferencia, aunque sus efectos se manifiestan allí (por ejemplo, inferencias erróneas o sesgadas).\n\nConclusión:\nEs una amenaza que se origina en el entrenamiento, pero impacta directamente la seguridad funcional en producción, especialmente en sistemas críticos.",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Manipulation of optimization algorithm",
    "Description": "Optimization algorithms are often used in the context of processes like Model Tuning to setup hyper-parameters values. Accordingly, nefarious abuse of such algorithms by adversaries may lead to erroneous tuning of models.",
    "Racional": "alteración maliciosa del proceso de optimización del modelo, afectando cómo se ajustan los parámetros durante el entrenamiento. Puede implicar manipulación de hiperparámetros, funciones de pérdida, o incluso el entorno de entrenamiento\n✅ Parcial\n\nJustificación: Ambas amenazas comparten el objetivo de comprometer el modelo durante su fase de entrenamiento. Sin embargo, ISO 6.7 se enfoca en la alteración directa del modelo, mientras que ENISA aborda la manipulación del proceso de optimización, que puede ser una técnica para lograr el envenenamiento del modelo. Es decir, la amenaza ENISA es más amplia y puede incluir el vector ISO como una de sus técnicas.\nManipulation of optimization algorithm (ENISA)\n└── Direct model poisoning (ISO 27090 6.7) [Parcial]\n--------------------------------------------------------------------------------\nTipo: Directa.\nJustificación: Ambas describen el mismo vector: manipulación directa del proceso de optimización del modelo, que es uno de los ejemplos más claros de Direct model poisoning.\n\nAunque ambas amenazas comparten el objetivo de comprometer el modelo durante el entrenamiento, no describen exactamente el mismo vector:\nManipulation of optimization algorithm (ENISA)\n└── Direct model poisoning (ISO 27090 6.7) [Parcial]",
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "1"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Misclassification based on adversarial examples",
    "Description": "This threat involves manipulation of model parameters or use of adversarial examples during inference to force misclassification of model results. This type of threats is related to the Actors category, as they have access to models and data sets, such is the case of Cloud providers, model providers and model users. Misclassification can also be instigated by Processes assets, such is the case of using adversarial examples during training and transfer learning stages, as well as during the inference stage.",
    "Racional": "Ocurre principalmente en la fase de inferencia, cuando se introducen entradas manipuladas para provocar errores de clasificación.\nManipulación de entradas en producción (fase de inferencia) para que el modelo clasifique incorrectamente. No requiere acceso al entrenamiento.\nEs una explotación en tiempo de ejecución. Provocar clasificaciones erróneas.\n- Inferencia: cuando se presentan entradas manipuladas para engañar al modelo. Variante específica que busca clasificaciones erróneas mediante ejemplos adversarios.\n\nSi se incluyen ejemplos adversarios en el entrenamiento, ya no estamos hablando de \"misclassification based on adversarial examples\", sino de una forma de \"data poisoning\".\nEn el caso de transfer learning, si el modelo base ya fue entrenado con ejemplos adversarios, la amenaza se clasifica como \"model poisoning\" o \"backdoor insertion\", no como misclassification por adversarial examples.\n\nEntrenamiento / Transferencia / Reentrenamiento: ⚠️ Técnicamente posible, pero la amenaza cambia de categoría:\n- En entrenamiento: sería data poisoning.\n- En transferencia: sería model poisoning o backdoor insertion.\n- En reentrenamiento: sería data poisoning en aprendizaje continuo.\n",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "1",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "1",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Model backdoors",
    "Description": "It is often the case that 3rd parties provide models (called as “teacher” models), previously trained and fine-tuned with large datasets that are useful to learn from small datasets and/or by organizations without access to high computational clusters. The resulting models may be subject to backdoor threats that expose their inner working (breach of confidentiality), impact their operation (integrity breach) or degrade/cancel their performance (impact on availability).",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "0",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "1",
    "6.11": "1",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Model poisoning",
    "Description": "This threat refers to a legitimate mode file being replaced entirely by a poisoned model file. In the context of AI as a Service, with many types of data and code being uploaded on cloud infrastructures, this threat may be realized by exploiting potential weaknesses of cloud providers.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "1",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Model Sabotage",
    "Description": "Sabotaging the model is a nefarious threat that refers to exploitation or physical damage of libraries and machine learning platforms that host or supply AI/ML services and systems.",
    "Racional": "Aunque el informe de ENISA no define explícitamente “Model Sabotage” como una categoría independiente en los fragmentos disponibles, el término se refiere generalmente a acciones maliciosas que degradan, corrompen o inutilizan el modelo de IA, ya sea durante el entrenamiento, despliegue o ejecución. Esto puede incluir envenenamiento, manipulación de parámetros, o interferencia en el entorno de ejecució\n✅ Directa\n\nJustificación: Ambos conceptos describen el mismo vector de amenaza: la alteración intencionada del modelo de IA para comprometer su integridad o funcionalidad. “Model Sabotage” en ENISA puede considerarse una categoría general que incluye el “Direct model poisoning” como técnica específica. En este caso, la relación es directa, ya que el sabotaje del modelo es el resultado buscado por el envenenamiento directo.\nModel Sabotage (ENISA)\n└── Direct model poisoning (ISO 27090 6.7) [Directa]",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "1",
    "6.10": "0",
    "6.11": "1",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Online system manipulation",
    "Description": "This is related to model replacement by a malicious backdoored model, used for targeted or non-targeted attack, which can be exploited by Actors like Cloud Providers during Processes like model training or transfer.",
    "Racional": "Manipulación activa del sistema en tiempo real, incluyendo entradas maliciosas.\t\nPuede incluir manipulación de inputs en producción.",
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "1",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Overloading/conf using labelled dataset",
    "Description": "Append attacks target availability by adding random samples to the training set to the point of preventing any model trained on that data set from computing any meaningful inference. The threat may lead to overfitting or underfitting the labelled dataset.",
    "Racional": "La amenaza descrita como “ataques de anexión” (append attacks) consiste en:\n\nAñadir muestras aleatorias o irrelevantes al conjunto de entrenamiento.\nProvocar sobreajuste (overfitting) o subajuste (underfitting).\nImpedir que el modelo aprenda patrones significativos.\nEsto encaja perfectamente con la definición de data poisoning en la ISO/IEC DIS 27090, sección 6.2, donde:\n\n“El atacante inserta, modifica o elimina datos en el conjunto de entrenamiento…”\n\nLa amenaza descrita como append attacks implica:\n\nAñadir muestras aleatorias o irrelevantes al conjunto de entrenamiento etiquetado.\nProvocar sobreajuste (overfitting) o subajuste (underfitting).\nImpedir que el modelo aprenda patrones significativos, afectando su capacidad de inferencia.\nEsto encaja perfectamente con la definición de data poisoning en la ISO/IEC DIS 27090, sección 6.2, que contempla:\n\n“Manipulación maliciosa de los datos de entrenamiento con el fin de influir negativamente en el aprendizaje del modelo…”\n\nConclusión:\nLa sobrecarga de conjuntos etiquetados con datos irrelevantes o maliciosos es una forma específica de data poisoning, orientada a comprometer la disponibilidad y funcionalidad del sistema de IA.\n\nEsta amenaza ocurre en:\n\nFase de entrenamiento inicial: Cuando se construye el modelo por primera vez.\nReentrenamiento continuo: Si el sistema incorpora nuevos datos sin validación robusta.\nTransferencia de modelos: Si se importa un modelo preentrenado con conjuntos contaminados.\nNo suele ocurrir en la fase de inferencia, ya que el ataque se basa en corromper el conjunto de entrenamiento, no en manipular entradas en tiempo real.",
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Reducing data accuracy",
    "Description": "This threat refers to the reduction of the degree of data accuracy, by directly modifying the data or by mixing datasets with highly different degrees of quality.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "1",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Reduce effectiveness of AI/ML results",
    "Description": "Users can make erroneous usage of AI services,either for not having a good understanding about the model capabilities or by not being able to understand when changes in the process imply model maintenance, and possibly re-training procedures. End-users can modify the input data to the model that results in \"de-training\" the model.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "1",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Sabotage",
    "Description": "Sabotage involves intentionally destroying or maliciously affecting the IT infrastructure that supports AI systems.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Scarce data",
    "Description": "AI relies on the availability of consistent and accessible data. This threat involves data scarcity (deliberately created by an adversary) that may compromise AI viability and/or compromise and limit its results. This can be exploited deliberately (for nefarious activities) or unintentionally during Data Ingestion.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Unauthorized access to data sets and data transfer process",
    "Description": "These threats are seen in context of storage of data sets in infrastructures provided by third parties, which make them remotely accessible. The threat refers to unauthorized access of the data stored and unauthorized access to the inner workings of the data transfer process.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "Unauthorized access to models’ code",
    "Description": "This threat refers to machine learning libraries and machine learning platforms being manipulated to inject malicious code that will exploit users models and gain access to datasets.",
    "Racional": "Amenaza de seguridad e integridad, que implica manipulación interna del código o librerías.\nManipulación de bibliotecas o plataformas de ML para inyectar código malicioso que explote los modelos y permita acceso a los datasets.\nDesarrollo / Integración\nTécnica:\tInyección de código malicioso\nObjetivo: \tAcceder a datasets o manipular el modelo\n\nML model confidentiality\n├── Unauthorized access to models’ code (acceso interno)\n└── Model Exfiltration (reconstrucción externa)\n--------------------------------------------------------------------------------------\nEsta amenaza se refiere al acceso no autorizado al código fuente del modelo, incluyendo el diseño, arquitectura, parámetros y lógica de funcionamiento. Puede ocurrir por vulnerabilidades en el entorno de desarrollo, despliegue o almacenamiento\n✅ Directa\n\nJustificación: Ambas amenazas describen el mismo vector: acceso no autorizado a los componentes internos del modelo de IA, ya sea como artefacto entrenado (ISO) o como código fuente (ENISA). En ambos casos, el objetivo es obtener el modelo sin permiso, lo que compromete la confidencialidad, la propiedad intelectual y puede facilitar ataques posteriores como evasión o manipulación.\nUnauthorized access to models’ code (ENISA)\n└── Direct model theft (ISO 27090 6.8) [Directa]",
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Nefarious Activity/Abuse",
    "Threat": "White-box, targeted or non-targeted",
    "Description": "This threat refers to misclassification to a specific target class or to a different class rather than the correct one. This type of threat is mainly associated with Processes assets like Model Selection/Building, Training, Testing, Transfer Learning and Model Deployment and can be exploited by Actors such as Model Providers.",
    "Racional": "Ocurre antes del despliegue, durante el diseño, construcción y validación del modelo\nEn escenarios white-box, el atacante tiene acceso al modelo, sus parámetros y arquitectura.\nEn ataques targeted, se busca que el modelo clasifique una entrada específica en una clase errónea determinada.\nEn ataques non-targeted, se busca simplemente que el modelo falle, sin importar la clase final.\n\n",
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Legal",
    "Threat": "Lack of data governance policies",
    "Description": "When personal data are processed, the existence of data governance policies is part of data controller’s accountability. The GDPR promotes the implementation of data protection by design measures as a way to be effective in the implementation of data protection principles, and in situation of high risks requires the implementation of a data protection impact assessment (DPIA). Data controllers should identify measurable goals and performance indicators that give evidence, also in a quantitative way, of their level of compliance with the principles and implement a DPIA as default option.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "0",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "1",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Legal",
    "Threat": "Compromise privacy during data operations",
    "Description": "Data manipulation or erroneous handling during Processes like Data Exploration' or Pre- Processing may lead to intentional or unintentional data breaches respectively and accordingly lead to legal concerns over privacy breaches",
    "Racional": "violación de la privacidad durante el procesamiento de datos en sistemas de IA, incluyendo:\n- Uso indebido de datos personales.\n- Falta de técnicas de preservación de privacidad (como anonimización o cifrado).\n- Exposición de datos sensibles durante inferencia, entrenamiento o despliegue\n\n✅ Parcial\n\nJustificación: Ambas amenazas están relacionadas con la exposición de información sensible, pero:\nISO 6.10 se enfoca en la entrada del modelo como canal de fuga.\nENISA aborda un espectro más amplio de violaciones de privacidad durante operaciones con datos, que puede incluir entradas, salidas, almacenamiento, y procesamiento.\nPor tanto, la amenaza ISO es un subconjunto específico de la categoría más general de ENISA.\n\nCompromise privacy during data operation (ENISA)\n└── Model input leak (ISO 27090 6.10) [Parcial]",
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Legal",
    "Threat": "Corruption of data indexes",
    "Description": "Data indexes threats manifest when their content becomes corrupted. Corruption may be the result of an attack, or due to system crashes or loss of network connectivity during index replication. The same events may cause interruptions of index construction tasks, bringing a partially built (and therefore defective) index to production. Also, running out of storage capacity during indexing or replication may cause an entire data index to be deleted. Denial-of-service attacks to indexes intentionally corrupt data indexes to decrease the performance of data access. Additionally, timing attacks to indexes use access time to public items before and after inserting them (which depend on the index content) to infer the presence and size of inaccessible data items.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Legal",
    "Threat": "Disclosure of personal information",
    "Description": "At all stages of the AI lifecycle, disclosure of personal information (either directly or by means of correlation) is a noteworthy threat. The threat is particularly manifested in the absence of verified data accuracy of sources, lack of data randomization, lack of pseudonymity mechanisms , etc.",
    "Racional": "Se refiere a la exposición directa de datos personales (PII) durante el procesamiento, almacenamiento o salida del sistema de IA.\nPuede ocurrir por fallos de configuración, falta de anonimización, errores en salidas (Sensitive model output), o accesos indebidos.\n (6.4 Membership inference) \n- Ataque específico que busca determinar si un registro concreto estuvo en el dataset de entrenamiento.\n- Es una inferencia estadística, no una fuga directa.\n\nDiferencia clave:\n- Disclosure of personal information = fuga directa (por salida, logs, mala configuración).\n- Membership inference = fuga indirecta (por inferencia a partir de respuestas del modelo).\nNo corresponde a 6.4, porque no implica inferencia estadística.\n------\nMembership inference revela si un registro estuvo en el entrenamiento.\nEsta amenaza se manifiesta particularmente en la ausencia de verificación de la precisión de las fuentes de datos, la falta de aleatorización de datos y la ausencia de mecanismos de seudonimización, entre otros.\nLa divulgación de información personal puede ocurrir en todas las fases:\n- Recolección y preprocesamiento: datos sin anonimización.\n- Entrenamiento: datasets mal gestionados.\n- Inferencia: salidas que revelan PII (Sensitive model output).\n\nDisclosure of personal information // es una amenaza general que incluye fugas directas\n└── Membership inference (ISO 6.4) // es una técnica que puede causar la amenaza si el dato inferido es personal. no es la técnica más común para esta amenaza\n\n",
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "1",
    "Environment /Tools": "1",
    "Processes": "1",
    "Actors": "1",
    "Models": "1",
    "Data": "1",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "1",
    "6.5": "0",
    "6.6": "0",
    "6.7": "1",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Legal",
    "Threat": "Lack of data protection compliance of 3rd parties",
    "Description": "Third parties are frequently used in providing and processing data, either directly or by means of libraries and models that they provide. This threat refers to the lack of compliance of the third parties with respect to applicable data protection regulations.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "1",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Legal",
    "Threat": "Profiling of end users",
    "Description": "Labeling may lend itself to a potential threat to anonymity and privacy by acting as a form of profiling.",
    "Racional": "Crear perfiles detallados de usuarios (hábitos, preferencias, características) a partir de datos o inferencias.\nPuede ocurrir en entrenamiento (si se usan datos personales) o operación (si se infieren atributos en tiempo real).\n\nUso de salidas del modelo para deducir características personales.\nCorrespondencia parcial: Relacionado con inferencia de atributos.\n\nModel inversion puede ser una técnica que habilite el profiling, porque permite inferir atributos sensibles que luego se usan para segmentar o perfilar usuarios.\nDiferencia clave:\n- Model inversion = técnica de ataque para extraer información.\n- Profiling = uso (legítimo o ilegítimo) de información para crear perfiles, no necesariamente mediante ataques.\n\nHay correlación funcional: Model inversion puede ser un vector para profiling, pero profiling no implica necesariamente model inversion (puede hacerse con datos explícitos).",
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Legal",
    "Threat": "SLA breach",
    "Description": "In the context of 3rd party dependencies, breach of contractual obligations and Service Level Agreements (SLAs) may lead to degradation of performance or even unavailability of the AI system to perform its operation.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "1",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Legal",
    "Threat": "Vendor lock-in",
    "Description": "When considering third parties to AI systems, e.g. cloud providers, data storage, AI libraries, etc. the threat of vendor lock-in involves the reliance on a sole third part provider without realistic alternative. While this might not constitute necessarily a cybersecurity threat, the lack of backup and overprovisioning might hamper operations.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "1"
  },
  {
    "Threat Category": "Legal",
    "Threat": "Weak requirements analysis",
    "Description": "AI requirements may fail when they are built in isolation from the social circumstances that make AI applications necessary. Specifically, functional requirements of AI systems about executing AI tasks with the needed accuracy may fail by not taking into account the impact of the corresponding inherent bias. Non-functional requirements of AI systems may fail by not considering the severity of information leaks and disclosures that can happen in virtualized high- performance execution environments, or when using untrusted software libraries. This is particularly dangerous for AI systems working in domains like healthcare, biotechnology, financial services and law.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Compromising AI application viability",
    "Description": "This type of threat refers to lack of understanding of what AI/ML are and how to succeed with the business models",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "0",
    "6. Model Selection / Building": "0",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "3rd party provider failure",
    "Description": "Failures or malfunctions of 3rd party providers, e.g. cloud providers, data storage providers, AI as Service providers, etc. may lead to unavailability of an AI system and improper or delayed operation",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "0",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Poor resource planning",
    "Description": "The proper functioning of an AI system may be compromised by the lack of adequate computational resources (storage capacity, transmission speed, computational power). This is particularly relevant in real time application and in the health sector. In order to deliver the expected beneficial outcome, it is essential that these resources are correctly dimensioned and allocated, and that the final user is aware of such infrastructural constraints. It is part of the informational accountability of the developer to provide user, and with prominent means, the list of resources to arrange, and their configuration settings, necessary to avoid failures and impacts on the functioning of an AI system.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "0",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "0",
    "9. Transfer Learning": "0",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Weak data governance policies",
    "Description": "In AI applications, data governance policies have been known to fail for defective data metrics, absence of documentation and lack of adaptability. Specifying data quality metrics for ML training is not straightforward, often leading to lack of numerical targets and insufficient documentation of data governance policies. Failure to monitor/record AI data usage (e.g. for training, testing or validation), and to update data governance policies based on AI systems achievements and failures is another common pitfall.",
    "Racional": "La amenaza de gobernanza débil de datos incluye:\n\nAusencia de métricas de calidad de datos.\nFalta de documentación sobre el uso de datos en entrenamiento, validación y testing.\nPolíticas no adaptadas a los cambios en el sistema de IA.\nEsto no constituye un ataque en sí, pero abre la puerta a ataques como el data poisoning, ya que:\n\nSin trazabilidad ni control de calidad, es más fácil que datos manipulados o corruptos se integren en el entrenamiento.\nLa falta de validación permite que etiquetas erróneas, datos espurios o ruido adversarial pasen desapercibidos.\n\nLa gobernanza débil de datos puede afectar múltiples fases del ciclo de vida de la IA:\n\nEntrenamiento: Si no se documenta ni valida el origen y calidad de los datos.\nTesting y validación: Si no se registran los resultados ni se ajustan las políticas.\nTransferencia de modelos: Si no se verifica la procedencia y trazabilidad del modelo importado.\nDespliegue y reentrenamiento: Si no se actualizan las políticas ante nuevos datos o fallos detectados.",
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "0",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Compromising ML pre-processing",
    "Description": "Flaws or defects of the data and metadata schemata greatly influence the quality of the analysis by applications that use the data. In AI applications, a flawed schema will negatively impact on the quality of the ingested information. Flaws often result from of inconsistencies in the use of modeling methodologies.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Corruption of data indexes",
    "Description": "Data indexes threats manifest when their content becomes corrupted. Corruption may be the result of an attack, or due to system crashes or loss of network connectivity during index replication. The same events may cause interruptions of index construction tasks, bringing a partially built (and therefore defective) index to production. Also, running out of storage capacity during indexing or replication may cause an entire data index to be deleted. Denial-of-service attacks to indexes intentionally corrupt data indexes to decrease the performance of data access. Additionally, timing attacks to indexes use access time to public items before and after inserting them (which depend on the index content) to infer the presence and size of inaccessible data items.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Compromise of model frameworks",
    "Description": "Model frameworks fail when are misconfigured or offer additional attack vectors with respect to traditional software, firmware and hardware enviroments. The ML platform’s data volume and and processing requirements mean that the workloads are often handled on the cloud, adding another level of complexity and vulnerability.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Errors or timely restrictions due to non-reliable data infrastructures",
    "Description": "This type of threat is related to data and computational exposure and/or inadequate capacity that may expose data and compromise privacy preservation.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Inadequate/absen t data quality checks",
    "Description": "Given the importance of data and the need for data to hold markers of their quality (e.g. in terms of sample size, variances, applied data collection methodologies, real vs synthetic data provenance), the lack of or the inadequacy of data quality checks may lead to poor performance of an AI system.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Label manipulation or weak labelling",
    "Description": "This threat refers to supervised learning systems, which not infer correctly due to wrong or imprecise data labels. If adversaries can only modify the training labels with some or all knowledge of the target model, they need to find the most vulnerable labels. Random perturbation of labels is one possible attack, while additionally there is the case of adversarial label noise (intentional switching of classification labels leading to deterministic noise, an error that the model cannot capture due to its generalization bias).",
    "Racional": "La amenaza descrita como manipulación o etiquetado débil incluye:\n\nModificación intencional de etiquetas para inducir errores sistemáticos.\nPerturbación aleatoria de etiquetas (ruido adversarial).\nEtiquetado impreciso o débil, que afecta la capacidad del modelo para aprender correctamente.\nEsto encaja perfectamente con la definición de data poisoning en la ISO/IEC DIS 27090, sección 6.2, donde se describe:\n\n“Manipulación maliciosa de los datos de entrenamiento con el fin de influir negativamente en el aprendizaje del modelo…”\n\nEsta amenaza ocurre típicamente en:\n\nFase de entrenamiento inicial: Cuando se construye el modelo por primera vez.\nFase de reentrenamiento o aprendizaje continuo: Si el sistema incorpora nuevos datos etiquetados sin validación robusta.\nTransferencia de aprendizaje: Si se importa un modelo preentrenado con etiquetas manipuladas.",
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "1",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Lack of documentation",
    "Description": "This threat generally manifests over the course of time. In AI systems, model selection should be made in a framework of accountability and trust and “black-box” approaches should be avoded. At any stage the choice of algorithm parameters should be justified and duly documented. Discarded alternatives should be disclosed and the consequences of model under-fitting or\noverfitting should be clearly explained. This set of parameters and design choices are important\nto be able to identify potential errors (intentional or unintentional). Failure to properly maintain\ndocumentation of the AI system threatens to indirectly limit its failsafe operation.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "ML Model Performance Degradation",
    "Description": "This performance of an AI’s system ML model may degrade due to the data governance policy,\nby omission or by corruption due to system crashes or loss of network connectivity.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Scarce data",
    "Description": "AI relies on the availability of consistent and accessible data. This threat involves data scarcity that may compromise AI viability and/or compromise and limit its results. This can be exploited deliberately (for nefarious activities) or unintentionally during Data Ingestion.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Stream interruption",
    "Description": "This threat relates to the interruption of data streams, during processes like data ingestion and training. The lack of data and data interruption, in case of stream processing, can cause failures in an AI/ML system",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Failures or malfunctions",
    "Threat": "Weak requirements analysis",
    "Description": "AI requirements may fail when they are built in isolation from the social circumstances that make AI applications necessary. Specifically, functional requirements of AI systems about executing AI tasks with the needed accuracy may fail by not taking into account the impact of the corresponding inherent bias. Non-functional requirements of AI systems may fail by not considering the severity of information leaks and disclosures that can happen in virtualized high- performance execution environments, or when using untrusted software libraries. This is particularly dangerous for AI systems working in domains like healthcare, biotechnology, financial services and law.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "1",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "1",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Eavesdropping Interception Hijacking",
    "Threat": "Data inference",
    "Description": "This threat may be exploited by the Data Providers and Model Providers, and can lad to inference of data. Evidently, in the case of personal data, such inference raises concerns in terms of privacy and/or discrimination. Riesgo de que un atacante infiera información sensible a partir del comportamiento del modelo.",
    "Racional": "Amenaza general que implica deducción de información sensible a partir del comportamiento del modelo. Puede incluir atributos, patrones, pertenencia, etc.\nUso de entradas para deducir información sensible del modelo o sus datos.\nOcurre durante la inferencia, aunque con fines de extracción.\n\nML model confidentiality\n├── Data inference\n│   └── Membership inference\n\nLa inferencia de pertenencia es una forma específica de inferencia de datos, centrada en la inclusión de una muestra en el conjunto de entrenamiento. \nTambién se considera una forma de fuga de información del modelo, ya que revela indirectamente aspectos del entrenamiento.\nSi la inferencia de datos se realiza a través del comportamiento del modelo, también compromete su confidencialidad.",
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "0",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "1",
    "6.4": "1",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Eavesdropping Interception Hijacking",
    "Threat": "Data theft",
    "Description": "This threat may manifest during the transportation of data, during Processes like Data Ingestion and in the context of access to data storage means. In these cases, data may be intercepted and stole.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "0",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "1",
    "6.13": "0"
  },
  {
    "Threat Category": "Eavesdropping Interception Hijacking",
    "Threat": "Model Disclosure",
    "Description": "Threat of leaking information about trained and/or tuned models internal parameters and other settings of models.  Exposición de la estructura interna del modelo, hiperparámetros, pesos, arquitectura, etc. Riesgo: Pérdida de propiedad intelectual, aumento de vulnerabilidad a ataques como model inversion o membership inference.",
    "Racional": "Model exfiltration es una técnica concreta dentro de esa categoría, centrada en la reconstrucción del modelo mediante consultas externas.\nObjetivo: Obtener parámetros, lógica o rendimiento\n\nML model confidentiality\n└── Model exfiltration (como técnica específica)\n\nML model confidentiality es una categoría amplia que incluye cualquier fuga de información interna del modelo\n\"Model Disclosure\" no se corresponde directamente con \"Model Exfiltration\".\nML model confidentiality\n├── Model Disclosure (fuga directa)\n└── Model Exfiltration (reconstrucción indirecta)\n--------------------------------------------------------------------------\n",
    "Confidentiality": "1",
    "Integrity": "0",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "0",
    "Actors": "0",
    "Models": "1",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "0",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "1",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Eavesdropping Interception Hijacking",
    "Threat": "Stream interruption",
    "Description": "This threat relates to the interruption of data streams, during processes like data ingestion and training. The lack of data and data interruption, in case of stream processing, can cause failures in an AI/ML system",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "0",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "0",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "0",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "0",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Eavesdropping Interception Hijacking",
    "Threat": "Weak encryption",
    "Description": "In the context of AI, this threat is related to assets of the Processes category, and refers to potential eavesdropping of data or hijacking of communications in the case of data transfers/storage/processing. The threat when materialized may expose data sets and even personal and sensitive information.",
    "Racional": null,
    "Confidentiality": "1",
    "Integrity": "1",
    "Availability": "0",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "1",
    "Actors": "0",
    "Models": "0",
    "Data": "1",
    "1. Business Goal Definition": "0",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Disasters",
    "Threat": "Environmental phenomena (heating, cooling, climate change)",
    "Description": "Environmental phenomena may adversely influence the operation of IT infrastructure and hardware systems that support AI systems. Climate change in particular has been consistently highlighted in ENISA reports on telecom incident reporting as the main cause of telecom outages. Such outages may lead to delays in decision-making, delays in the processing of data streams and entire AI systems being placed offline.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  },
  {
    "Threat Category": "Disasters",
    "Threat": "Natural disasters (earthquake, flood, fire, etc.",
    "Description": "Natural disasters may lead to unavailability or destruction of the IT infrastructures and hardware that enables the operation, deployment and maintenance of AI systems.",
    "Racional": null,
    "Confidentiality": "0",
    "Integrity": "0",
    "Availability": "1",
    "Artefacts": "0",
    "Environment /Tools": "1",
    "Processes": "0",
    "Actors": "0",
    "Models": "0",
    "Data": "0",
    "1. Business Goal Definition": "1",
    "2. Data Ingestion": "1",
    "3. Data Exploration": "1",
    "4. Data Pre-processing": "1",
    "5. Feature Selection": "1",
    "6. Model Selection / Building": "1",
    "7. Model Training": "1",
    "8. Model Tuning": "1",
    "9. Transfer Learning": "1",
    "10. Model Deployment": "1",
    "11. Model Maintenance": "1",
    "12. Business Understanding": "1",
    "6.2": "0",
    "6.3": "0",
    "6.4": "0",
    "6.5": "0",
    "6.6": "0",
    "6.7": "0",
    "6.8": "0",
    "6.9": "0",
    "6.10": "0",
    "6.11": "0",
    "6.12": "0",
    "6.13": "0"
  }
]